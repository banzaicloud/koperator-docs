---
title: Securing Kafka With SSL
shorttitle: SSL
weight: 300
---

The {{< kafka-operator >}} makes securing your Apache Kafka cluster with SSL simple.

## Enable SSL encryption in Apache Kafka {#enable-ssl}

To create an Apache Kafka cluster with SSL encryption enabled, you must enable SSL encryption and configure the secrets in the **listenersConfig** section of your **KafkaCluster** Custom Resource. You can provide your own CA cert and CA key, or instruct the operator to create them for you from your cluster configuration. Using **ssLSecrets** koperator  generates client and server certificates signed by the provided or genetared CA. Listeners will share the same server certificate. The client certificate will be used by the koperator, cruise control and cruise control metrics reporter to communicate with listener on SSL which is used for inner broker communication.
    
From koperator version and kafka version* You can also add your own generated certificates per listener. You can use hybrid solutions like you add your own generated certificate for one external listener and other listeners can use the auto generated one. See details below.

## Using auto generated certificates ( **ssLSecrets** )
{{< include-headless "warning-listener-protocol.md" "kafka-operator" >}}

The following example enables SSL and automatically generates the certificates:

{{< include-code "enable-ssl.sample" "yaml" >}}

If `sslSecrets.create` is `false`, the operator will look for the secret at `sslSecrets.tlsSecretName` and expect these values:

| Key          | Value              |
|:------------:|:-------------------|
| `caCert`     | The CA certificate |
| `caKey`      | The CA private key |

## Using own certificates
### For listener which is not used for inner broker communication
In this **KafkaCluster** custom resource (https://github.com/banzaicloud/koperator/blob/master/config/samples/kafkacluster_with_ssl_hybrid_customcert.yaml) SSL is enabled for all listeners and certificates will be auto generated for "inner" and "controller" listeners. The "external" and "internal" listeners will use the user provided certificates. **ServerSSLCertSecret** is a reference to the kubernetes secret that contains the server certificate for the listener to be used for SSL communication.
In the server secret these keys - values must be set:

| Key              | Value                                     |
|:----------------:|:------------------------------------------|
| `keystore.jks`   | Certificate and private key in JKS format |
| `truststore.jks` | Truested CA certificate in JKS format     |
| `password`       | Password for the key and trust store      |

Koperator using JKS format based certificate for listener config. 
### For listener which is used for inner broker communication
In this **KafkaCluster** custom resource (https://github.com/banzaicloud/koperator/blob/master/config/samples/kafkacluster_with_ssl_groups_customcert.yaml ) SSL is enabled for all listeners and user provided certificates used. In that case when custom certificate is used for listener which is used for inner broker communication than client certificate need to be specified also. Client certificate will be used by Koperator, Cruise Control, Cruise Control Metrics Reporter to communicate on SSL. **ClientSSLCertSecret** is a reference to the kubernetes secret where custom client SSL certificate can be provided. Client secret data keys must be the same as the server secret. Client certificate must be signed by the same CA authority as the server certificate for the corresponding listener. **ClientSSLCertSecret** has to be in the **KafkaCluster** custom resource spec field. 

### Generate JKS certificate 
JKS format certificates can be generated by openssl and keystore applications. This script also can be used to generate them: https://github.com/confluentinc/confluent-platform-security-tools/blob/master/kafka-generate-ssl.sh.
Kafka listeners using 2-way-SSL mutual authentication. This is why important to set properly the CNAME (Common Name) and if needed the SAN (Subject Alternative Name) fields in certificates. In the following description we assume that the kafka cluster is in the kafka namespace. 
For the client certificate CNAME must be "kafka-controller.kafka.mgt.cluster.local". (where .kafka. is the namespace of the kafka cluster) 
For internal listeners which are exposed by a headless service (kafka-headless) CNAME must be "kafka-headless.kafka.svc.cluster.local" and SAN field must contains the following:
- *.kafka-headless.kafka.svc.cluster.local
- kafka-headless.kafka.svc.cluster.local
- *.kafka-headless.kafka.svc
- kafka-headless.kafka.svc
- *.kafka-headless.kafka
- kafka-headless.kafka
- kafka-headless
For internal listeners which are exposed by a normal service (kafka-all-broker) CNAME must be "kafka-all-broker.kafka.svc.cluster.local"
For external listener you need to use the advertised load balancer hostname as CNAME. The hostname need to be specified in the **KafkaCluster** custom resource with **hostnameOverride** and the **accessMethod** has to be "LoadBalancer". You can read more about this override here (https://sdm-docs.eticloud.io/docs/kafka-operator/external-listener/?hostName#loadbalancer 5th section).


## Using Kafka ACLs with SSL

> Note: {{< kafka-operator >}} provides only basic ACL support. For a more complete and robust solution, consider using the [Streaming Data Manager](https://banzaicloud.com/products/supertubes/) product.
> {{< include-headless "kafka-operator-supertubes-intro.md" >}}

If you choose not to enable ACLs for your Apache Kafka cluster, you may still use the `KafkaUser` resource to create new certificates for your applications.
You can leave the `topicGrants` out as they will not have any effect.

1. To enable ACL support for your Apache Kafka cluster, pass the following configurations along with your `brokerConfig`:

    ```yaml
    authorizer.class.name=kafka.security.authorizer.AclAuthorizer
    allow.everyone.if.no.acl.found=false
    ```

1. The operator will ensure that cruise control and itself can still access the cluster, however, to create new clients
you will need to generate new certificates signed by the CA, and ensure ACLs on the topic. The operator can automate this process for you using the `KafkaUser` CRD.
    For example, to create a new producer for the topic `test-topic` against the KafkaCluster `kafka`, apply the following configuration:

    ```bash
    cat << EOF | kubectl apply -n kafka -f -
    apiVersion: kafka.banzaicloud.io/v1alpha1
    kind: KafkaUser
    metadata:
      name: example-producer
      namespace: kafka
    spec:
      clusterRef:
        name: kafka
      secretName: example-producer-secret
      topicGrants:
        - topicName: test-topic
          accessType: write
    EOF
    ```

    This will create a user and store its credentials in the secret `example-producer-secret`. The secret contains these fields:

    | Key          | Value                |
    |:------------:|:---------------------|
    | `ca.crt`     | The CA certificate   |
    | `tls.crt`    | The user certificate |
    | `tls.key`    | The user private key |

1. You can then mount these secrets to your pod. Alternatively, you can write them to your local machine by running:

    ```bash
    kubectl get secret example-producer-secret -o jsonpath="{['data']['ca\.crt']}" | base64 -d > ca.crt
    kubectl get secret example-producer-secret -o jsonpath="{['data']['tls\.crt']}" | base64 -d > tls.crt
    kubectl get secret example-producer-secret -o jsonpath="{['data']['tls\.key']}" | base64 -d > tls.key
    ```

1. To create a consumer for the topic, run this command:

    ```bash
    cat << EOF | kubectl apply -n kafka -f -
    apiVersion: kafka.banzaicloud.io/v1alpha1
    kind: KafkaUser
    metadata:
      name: example-consumer
      namespace: kafka
    spec:
      clusterRef:
        name: kafka
      secretName: example-consumer-secret
      includeJKS: true
      topicGrants:
        - topicName: test-topic
          accessType: read
    EOF
    ```

1. The operator can also include a Java keystore format (JKS) with your user secret if you'd like. Add `includeJKS: true` to the `spec` like shown above, and then the user-secret will gain these additional fields:

    | Key                     | Value                |
    |:-----------------------:|:---------------------|
    | `tls.jks`               | The java keystore containing both the user keys and the CA (use this for your keystore AND truststore) |
    | `pass.txt`              | The password to decrypt the JKS (this will be randomly generated) |
